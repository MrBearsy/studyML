# 深度学习实战笔记

## 第一章 深度学习简要介绍

**深度学习影响因素**

1. 大数据，数据量的大小决定深度神经网络分类和预测准确度
2. 网络架构，前馈、卷积、循环的组合架构
3. 训练方式
4. GPU 大规模张量计算

**深度学习成功的原因**

1. 特征学习，自动提取特征信息
2. 迁移学习，把训练好的神经网络迁移到另一个神经网络中去

## 第二章 PyTorch简介

**PyTorch三个关键特性**

1. 与Python完美融合
2. 支持张量计算
3. 动态计算图

**张量计算**

![image-20241210131226870](picture/image-20241210131226870.png)



三维张量 (a,b,c) a是矩阵个数，b是行，c是列

张量访问 x[1,2]，二维张量第二行第三列，x[:,2] 第三列所有元素

**张量与NumPy数组之间的转换**

a.numpy()

torch.from_numpy(b)

torch.FloatTensor(b) float类型

torch.LongTensor(b) int类型



**GPU上的张量运输**

torch.cuda.is_available()

x=x.cuda()   把tensor放在gpu上

y=y.cuda()    

x=x.cpu()     把tensor放在cpu上



**动态计算图**

深度学习的核心是利用反向传播算法，精确计算网络每一个单元对网络的贡献，现在大多数深度学习框架采用了计算图技术，不需要为每一种架构的网络定制不同的反向传播算法，只需要关注如何实现神经网络的前馈运输即可。前馈运输完成后，深度学习框架会自动搭建计算图，让反向传播算法自动运行。计算图技术的出现大幅度提升了构建神经技术系统的效率。

*计算图的构成*

1. 变量variable
2. 运算computation
3. 有向连边 表示节点的因果和依赖关系

如图所示

![image-20241210140502235](picture/image-20241210140502235.png)

箭头方向表示输入来源，方框节点为变量，椭圆节点为运输操作

PyTorch借助自动微分变量实现动态计算图的，自动微分变量和普通的张量表面没区别，但内部数据结构比张量复杂，目前这两个变成了一个，每一个张量都是一个自动微分变量。

自动微分变量有三部分组成，data、grad、grad_fn

data是普通张量，grad_fn是存储计算路径，用于访问计算图上一个节点，调用.backward()后，导数值存储在 .grad中

示例：

```python
#Python

x=torch.ones(2,2,requires_grad=True)

y=x+2

z=y*y

t=torch.mean(z)

t.backward()

z.grad; y.grad; x.grad

```

PyTorch规定，只有计算图的叶节点才可以通过.backward()获得梯度信息

------

```python
a=torch.rand(1,requires_grad=True)
predictions=a.expand_as(x_train)*x_train
expand_as(x)# 可以升维

a = torch.rand(1, requires_grad = True)
b = torch.rand(1, requires_grad = True)
predictions = a.expand_as(x_train) * x_train + b.expand_as(x_train)
predictions

loss = torch.mean((predictions - y_train) ** 2)  #计算损失函数
loss.backward() #开始反向传播梯度

#开始梯度下降，其中0.001为学习率
a.data.add_(- 0.001 * a.grad.data) 
b.data.add_(- 0.001 * b.grad.data)

#注意我们无法改变一个tensor，而只能对tensor的data属性做更改
#所有函数加“_”都意味着需要更新调用者的数值

a.grad.data.zero_()
b.grad.data.zero_()


```

